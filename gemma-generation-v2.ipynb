{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:05:27.681503Z","iopub.execute_input":"2024-04-09T17:05:27.682281Z","iopub.status.idle":"2024-04-09T17:05:27.686476Z","shell.execute_reply.started":"2024-04-09T17:05:27.682221Z","shell.execute_reply":"2024-04-09T17:05:27.685590Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[\"Text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Text'][100:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data['Text'][22]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\ntoken = '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'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",token=token, device_map='cuda')\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token=token, device_map='cuda')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# prompt_template = \"Generate five questions based on the following statement: '{}'. Question:\"\n\n# input_string = x\n\n# prompt = prompt_template.format(input_string)\n\n# input_ids = tokenizer(prompt, return_tensors=\"pt\")\n\n# outputs = model.generate(**input_ids, max_new_tokens=1000)\n# # print(tokenizer.decode(outputs[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor n,i in enumerate(data['Text'][251:]):\n    start = time.time()\n# model generuje\n    try:\n        prompt_template = \"Generate five questions based on the following statement: '{}'. Question:\"\n\n        input_string = i\n\n        prompt = prompt_template.format(input_string)\n\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n\n        outputs = model.generate(**input_ids, max_new_tokens=1000)\n\n        input_string = tokenizer.decode(outputs[0])\n        print(n)\n\n        # skrypt do ucinania\n\n        question_index = input_string.find(\"Question:\")\n        if question_index != -1:  \n\n            cropped_string = input_string[question_index:]\n            print(cropped_string)\n            output_dir = \"/kaggle/working/\" \n\n            output_file = output_dir + f\"/question{n}.txt\"\n            with open(output_file, 'w') as outfile:\n                outfile.write(cropped_string)\n        else:\n            print(\"bad\")\n        end = time.time()\n        print(end - start)\n    except:\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}